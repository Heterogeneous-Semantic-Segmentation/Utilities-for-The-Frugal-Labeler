{
 "cells": [
  {
   "source": [
    "## Multi-class semanticsegmentation on heterogeneous labels\n",
    "This is an exemplary Notebook which demonstrates the core functionality of [Methods for the frugal labeler: Multi-class semanticsegmentation on heterogeneous labels](https://osf.io/uyk79/) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imports\n",
    "\n",
    "from pymodules.create_one_hot_encoded_map_from_mask import get_one_hot_map\n",
    "from pymodules.adaptive_objective_functions import adaptive_dice_loss,ca_loss\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from os import walk\n",
    "%env SM_FRAMEWORK=tf.keras\n",
    "import segmentation_models as sm\n",
    "import numpy as np\n",
    "from pymodules.data import train_generator,test_generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from pymodules.unet_model import unet\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import  EarlyStopping, ReduceLROnPlateau \n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import keras\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "**Important:** the dataset needs to be downloaded in advance and be provided under `data/`. The dataset can be found here: [data.zip](https://osf.io/c3ut5/) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# color map is different on test data\n",
    "col_map = [[255,255,255],[20,20,20],[19,19,19],[0,0,0]]\n",
    "\n",
    "X_test = []\n",
    "for filepath in os.listdir('data/test_images/ventral_samples_R0004'):\n",
    "    image = Image.open('data/test_images/ventral_samples_R0004/'+filepath)\n",
    "    image = image.resize((256, 256))\n",
    "    # convert image to numpy array\n",
    "    data = np.asarray(image)\n",
    "    data = data/255.\n",
    "    X_test.append(data)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = []\n",
    "for filepath in os.listdir('data/test_images/ventral_mask_combined_R0004'):\n",
    "    image = Image.open('data/test_images/ventral_mask_combined_R0004/'+filepath)\n",
    "    image = image.resize((256, 256)) \n",
    "    Y_test.append(get_one_hot_map(np.asarray(image),col_map))\n",
    "Y_test = tf.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_masks = []\n",
    "classes_color_dict = {0:[0,150,130],1:[64,64,64],2:[255,255,255],3:[0,0,0]}\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        predicted = model.predict(X_test[1].reshape(1,256,256,3))\n",
    "        learned_masks.append(predicted[0])\n",
    "        disp_array = np.repeat(np.zeros(list(predicted[0].shape[:2])).reshape(256,256,1),3,axis=2)\n",
    "        for key in classes_color_dict:\n",
    "            true_values = np.full(list(predicted[0].shape[:2]) + [3],classes_color_dict.get(key))\n",
    "            disp_array = np.where(np.repeat((np.argmax(predicted[0],axis=2) == key).reshape(256,256,1),3,axis=2),true_values,disp_array)\n",
    "        f, axarr = plt.subplots(1,2)\n",
    "        axarr[0].imshow(X_test[1])\n",
    "        axarr[1].imshow(disp_array.astype(int))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_directory = './missing_labels_test'\n",
    "\n",
    "batch_size = 11\n",
    "epochs = 250\n",
    "iterations_per_epoch = 250\n",
    "data_gen_args = dict(rotation_range=0.3,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.1,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "missing_ratio = 0.0\n",
    "for missing_ratio in [0.0,0.3,0.7]:\n",
    "    \n",
    "    # Clear learned masks\n",
    "    learned_masks = []\n",
    "    \n",
    "    print('---------------------')\n",
    "    print('Missing Ratio: %f'%missing_ratio)\n",
    "    print('---------------------')\n",
    "\n",
    "    Path('%s/%s'%(save_directory,missing_ratio)).mkdir(exist_ok=True)\n",
    "\n",
    "    train_gen = train_generator(batch_size=batch_size,\n",
    "                                      train_path='data/train_images',\n",
    "                                      image_folder='ventral_samples',\n",
    "                                      mask_folders=['ventral_mask_atrium', 'ventral_mask_bulbus', 'ventral_mask_heart'],\n",
    "                                      heterogeneously_labeled_masks=['ventral_mask_atrium', 'ventral_mask_bulbus',\n",
    "                                                                     'ventral_mask_heart'],\n",
    "                                      missing_labels_ratio=missing_ratio,\n",
    "                                      aug_dict=data_gen_args,\n",
    "                                      image_color_mode='rgb',\n",
    "                                      target_size=(256, 256))\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    val_gen = val_datagen.flow(X_test, Y_test, batch_size=batch_size)\n",
    "    model = unet(adaptive_dice_loss,input_size = (256,256,3),output_filters=4)\n",
    "    model_checkpoint = ModelCheckpoint('%s/%s/weights_custom_loss.hdf5'%(save_directory,missing_ratio), monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\",verbose = 1,mode='min',patience=30)\n",
    "    reduce_lr =  ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.5, patience = 10,verbose = 0, mode = \"auto\", epsilon = 1e-04, cooldown = 0,min_lr = 1e-5)\n",
    "    history = model.fit_generator(train_gen,steps_per_epoch=iterations_per_epoch,epochs=epochs,callbacks=[model_checkpoint,CustomCallback(),early_stopping,reduce_lr],validation_data=val_gen,verbose=1)\n",
    "\n",
    "    with open('%s/%s/history'%(save_directory,missing_ratio), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    with open('%s/%s/learned_masks'%(save_directory,missing_ratio), 'wb') as file_pi:\n",
    "        pickle.dump(learned_masks, file_pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}